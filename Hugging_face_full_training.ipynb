{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nghiauet/Using_LLaMA_FAISS_and_LangChain_for_Question-Answering/blob/master/Hugging_face_full_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4FB1zLFb1v2"
      },
      "source": [
        "# A full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JYPx4jab1v5"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sG9bv-9sb1v5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: transformers[sentencepiece]\n",
            "Requirement already satisfied: accelerate in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (0.22.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from accelerate) (2.0.1)\n",
            "Requirement already satisfied: filelock in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (68.0.0)\n",
            "Requirement already satisfied: wheel in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.41.0)\n",
            "Requirement already satisfied: cmake in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install datasets evaluate transformers[sentencepiece]\n",
        "# !pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "syq18VE7b1v7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading builder script: 100%|██████████| 28.8k/28.8k [00:00<00:00, 10.9MB/s]\n",
            "Downloading metadata: 100%|██████████| 28.7k/28.7k [00:00<00:00, 19.6MB/s]\n",
            "Downloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 15.4MB/s]\n",
            "Downloading data: 6.22kB [00:00, 4.55MB/s]/3 [00:00<?, ?it/s]\n",
            "Downloading data: 1.05MB [00:00, 13.7MB/s]/3 [00:00<00:01,  1.78it/s]\n",
            "Downloading data: 441kB [00:00, 30.9MB/s]2/3 [00:00<00:00,  2.22it/s]\n",
            "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]\n",
            "Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 29071.25 examples/s]\n",
            "Generating validation split: 100%|██████████| 408/408 [00:00<00:00, 15631.52 examples/s]\n",
            "Generating test split: 100%|██████████| 1725/1725 [00:00<00:00, 77745.74 examples/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 2.21kB/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 48.1kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 472kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.88MB/s]\n",
            "Map: 100%|██████████| 3668/3668 [00:00<00:00, 44994.13 examples/s]\n",
            "Map: 100%|██████████| 408/408 [00:00<00:00, 43574.97 examples/s]\n",
            "Map: 100%|██████████| 1725/1725 [00:00<00:00, 51016.96 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "import time\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dV_iSIzgb1v7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "tokenized_datasets[\"train\"].column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "shRlt1t7b1v7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['attention_mask', 'input_ids', 'labels', 'token_type_ids']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "exT7Rs9wb1v8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z7f03mkib1v8",
        "outputId": "276fd2d5-5e2c-4b1e-edb9-0041b18adb9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'labels': torch.Size([8]),\n",
              " 'input_ids': torch.Size([8, 81]),\n",
              " 'token_type_ids': torch.Size([8, 81]),\n",
              " 'attention_mask': torch.Size([8, 81])}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for batch in train_dataloader:\n",
        "    break\n",
        "{k: v.shape for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aDA6u8ifb1v9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading model.safetensors: 100%|██████████| 440M/440M [00:07<00:00, 59.2MB/s] \n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l65xffpcb1v9",
        "outputId": "dbfcd78f-26ed-4581-e952-4956554ee05f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6127, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
          ]
        }
      ],
      "source": [
        "outputs = model(**batch)\n",
        "print(outputs.loss, outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g5fqN-oJb1v-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QP84_2ITb1v-",
        "outputId": "2293f157-f60e-41d3-fc1d-2f78e1f4294f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1377\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "print(num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZeuiivLFb1v-",
        "outputId": "50ac9854-9b72-4d95-9b70-b2b2387227d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KVGKDkXjb1v-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1374/1377 [00:46<00:00, 29.63it/s]"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "start_time = time.time()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "end_time = time.time()\n",
        "print(\"Training complete!\")\n",
        "print(f\"Time taken: {end_time - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "T237_yPIb1v_",
        "outputId": "a5d46a67-2101-45f7-d509-266ad83117eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 100%|██████████| 5.75k/5.75k [00:00<00:00, 8.74MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.8455882352941176, 'f1': 0.8934010152284263}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bQ0Ncss6b1v_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 1377/1377 [09:13<00:00,  2.49it/s]\n",
            "100%|██████████| 1377/1377 [00:46<00:00, 30.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training complete!\n",
            "Time taken: 46.17062568664551 seconds\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
        "# import format_time\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "# calculate elapsed time\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "# calculate elapsed time\n",
        "end_time = time.time()\n",
        "print(\"Training complete!\")\n",
        "print(f\"Time taken: {end_time - start_time} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "key in batch:  dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])\n",
            "shape of each key:  {'labels': torch.Size([8]), 'input_ids': torch.Size([8, 89]), 'token_type_ids': torch.Size([8, 89]), 'attention_mask': torch.Size([8, 89])}\n"
          ]
        }
      ],
      "source": [
        "# print first batch in train_dataloader\n",
        "for batch in train_dataloader:\n",
        "    print('key in batch: ', batch.keys())\n",
        "    print('shape of each key: ', {k: v.shape for k, v in batch.items()})\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sAmtYinfb1v_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 1377/1377 [00:49<00:00, 27.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training complete!\n",
            "Time taken: 50.7180118560791 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "from accelerate import Accelerator\n",
        "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
        "\n",
        "accelerator = Accelerator()\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "train_dl, eval_dl, model, optimizer = accelerator.prepare( # prepare model for accelerator\n",
        "    train_dataloader, eval_dataloader, model, optimizer\n",
        ")\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dl)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "start_time = time.time()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dl:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "end_time = time.time()\n",
        "print(\"Training complete!\")\n",
        "print(f\"Time taken: {end_time - start_time} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GEC_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
