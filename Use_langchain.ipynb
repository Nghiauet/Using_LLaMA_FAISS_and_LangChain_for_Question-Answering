{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers\n",
    "# !pip install python-dotenv\n",
    "\n",
    "# load variable from .env \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:1006: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]\n",
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "hf_auth  = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
       " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = generate_text(\"Explain me the difference between Data Lakehouse and Data Warehouse.\")\n",
    "# print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing HF Pipeline in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " everybody wants to be an entrepreneur, but not everyone knows how to succeed in entrepreneurship. Here are some tips that can help you on your path to success:\n",
      "1. Identify a problem or need in the market and create a solution for it. This is the foundation of any successful business. You must identify a gap in the market and find a way to fill it with a product or service that meets the needs of potential customers.\n",
      "2. Develop a unique value proposition. Your business should offer something different from what's already out there. Find a niche and own it. This will help you stand out from the competition and attract customers who are looking for something new and innovative.\n",
      "3. Build a strong brand identity. Your brand is how people perceive you, so make sure it's consistent across all channels. Develop a visual identity that reflects your values and personality, and use it consistently across all of your marketing materials, including your website, social media profiles, and packaging.\n",
      "4. Create a solid business plan. A good business plan will help you stay focused and on track as you build your business. It should include a detailed description of your product or service, your target market, your pricing strategy, and your financial projections.\n",
      "5. Network and build relationships. Building a network of contacts in your industry can help you gain access to valuable resources, such as funding, mentorship, and partnerships. Attend industry events, join relevant organizations, and connect with other entrepreneurs and professionals in your field.\n",
      "6. Be persistent and adaptable. Starting a business is hard work, and it won't always go according to plan. You may encounter setbacks and challenges along the way, but don't give up. Stay flexible and willing to pivot when necessary, and keep pushing forward until you achieve your goals.\n",
      "7. Focus on customer acquisition and retention. Acquiring new customers is important, but retaining them is just as crucial. Make sure you have a plan in place to keep your customers coming back for more, whether through loyalty programs, repeat purchases, or excellent customer service.\n",
      "8. Continuously learn and improve. The entrepreneurial journey is never-ending, and there is always room for growth and improvement. Keep learning about your industry, your customers, and yourself, and use that knowledge to improve your products, services,\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# checking again that everything is working fine\n",
    "text = llm(prompt=\"How to succeed in entrepreneurship?\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web_links = [\"https://www.databricks.com/\",\"https://help.databricks.com\",\"https://databricks.com/try-databricks\",\"https://help.databricks.com/s/\",\"https://docs.databricks.com\",\"https://kb.databricks.com/\",\"http://docs.databricks.com/getting-started/index.html\",\"http://docs.databricks.com/introduction/index.html\",\"http://docs.databricks.com/getting-started/tutorials/index.html\",\"http://docs.databricks.com/release-notes/index.html\",\"http://docs.databricks.com/ingestion/index.html\",\"http://docs.databricks.com/exploratory-data-analysis/index.html\",\"http://docs.databricks.com/data-preparation/index.html\",\"http://docs.databricks.com/data-sharing/index.html\",\"http://docs.databricks.com/marketplace/index.html\",\"http://docs.databricks.com/workspace-index.html\",\"http://docs.databricks.com/machine-learning/index.html\",\"http://docs.databricks.com/sql/index.html\",\"http://docs.databricks.com/delta/index.html\",\"http://docs.databricks.com/dev-tools/index.html\",\"http://docs.databricks.com/integrations/index.html\",\"http://docs.databricks.com/administration-guide/index.html\",\"http://docs.databricks.com/security/index.html\",\"http://docs.databricks.com/data-governance/index.html\",\"http://docs.databricks.com/lakehouse-architecture/index.html\",\"http://docs.databricks.com/reference/api.html\",\"http://docs.databricks.com/resources/index.html\",\"http://docs.databricks.com/whats-coming.html\",\"http://docs.databricks.com/archive/index.html\",\"http://docs.databricks.com/lakehouse/index.html\",\"http://docs.databricks.com/getting-started/quick-start.html\",\"http://docs.databricks.com/getting-started/etl-quick-start.html\",\"http://docs.databricks.com/getting-started/lakehouse-e2e.html\",\"http://docs.databricks.com/getting-started/free-training.html\",\"http://docs.databricks.com/sql/language-manual/index.html\",\"http://docs.databricks.com/error-messages/index.html\",\"http://www.apache.org/\",\"https://databricks.com/privacy-policy\",\"https://databricks.com/terms-of-use\"] \n",
    "\n",
    "loader = WebBaseLoader(web_links)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of documents: <class 'list'>\n",
      "shape of documents: 39\n",
      "doc_example : Data Lakehouse Architecture and AI Company | DatabricksSkip to main contentPlatformThe Databricks Lakehouse PlatformDelta LakeData GovernanceData EngineeringData StreamingData WarehousingData SharingMachine LearningData SciencePricingMarketplaceOpen source techSecurity and Trust CenterDiscover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform\n",
      "Read nowSolutionsSolutions by IndustryFinancial ServicesHealthcare and Life SciencesManufacturingCommunications, Media & EntertainmentPublic SectorRetailSee all IndustriesSolutions by Use CaseSolution AcceleratorsProfessional ServicesDigital Native BusinessesData Platform MigrationReport\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report\n",
      "Read nowLearnDocumentationTraining & CertificationDemosResourcesOnline CommunityUniversity AllianceEventsData + AI SummitBlogLabsBeaconsExecutive InsightsMissed Data + AI Summit?\n",
      " \n",
      "Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand.Watch on demandCustomersPartnersCloud PartnersAWSAzureGoogle CloudPartner ConnectTechnology and Data PartnersTechnology Partner ProgramData Partner ProgramBuilt on Databricks Partner ProgramConsulting & SI PartnersC&SI Partner ProgramPartner SolutionsConnect with validated partner solutions in just a few clicks.Learn moreCompanyCareers at DatabricksOur TeamBoard of DirectorsCompany BlogNewsroomDatabricks VenturesAwards and RecognitionContact UsSee why Gartner named Databricks a Leader for the second consecutive yearGet the reportTry DatabricksWatch DemosContact UsLoginJoin Generation AIConnect with like-minded peers and companies who believe in the transformative power of data, analytics and AI.Explore eventsThe best data warehouse is a lakehouseUnify all your data, analytics and\n",
      "AI on one platformGet started for freeLearn moreCut costs and speed up innovation with the Lakehouse PlatformLearn moreUnifiedOne platform for your data, consistently governed and available for all your analytics and AIOpenOpen standards provide easy integration with other tools plus secure, platform-independent data sharingScalableScale efficiently with every workload from simple data pipelines to massive LLMsData-driven organizations choosing Lakehouse\n",
      "See all customers\n",
      "Lakehouse unifies your data teamsData sharingOpen data sharingEasily collaborate with anyone on any platform with the first open approach to data sharing. Share live data sets, models, dashboards and notebooks while maintaining strict security and governance.Learn more Watch demoData management and engineeringStreamline your data ingestion and managementWith automated and reliable ETL, open and secure data sharing, and lightning-fast performance, Delta Lake transforms your data lake into the destination for all your structured, semi-structured and unstructured data.Learn more Watch demoData warehousingDerive new insights from the most complete dataWith ready access to the freshest and most complete data and the power of Databricks SQL — up to 12x better price/performance than traditional cloud data warehouses \u0000— data analysts and scientists can now quickly derive new insights.Learn more Watch demoData science and machine learningAccelerate ML across the entire lifecycleThe lakehouse forms the foundation of Databricks Machine Learning — a data-native and collaborative solution for the full machine learning lifecycle, from featurization to production. Combined with high-quality, highly performant data pipelines, lakehouse accelerates machine learning and team productivity.Learn more Watch demoData governanceUnify governance for data, analytics and AIMaintain a compliant, end-to-end view of your data estate with a single model of data governance for all your structured and unstructured data. Centralize auditing and track usage through automated lineage and monitoring capabilities.Learn more Watch demoThe data warehouse is history. Discover why the lakehouse is the modern architecture for data and AI.Discover LakehouseJoin Generation AIAugust–November 2023Explore the latest in data, analytics and AI on Databricks Lakehouse. Connect with local data teams at a Data + AI World Tour stop near you.Register nowData + AI in the real worldDiscover the latest trends in data science and AI adoption across 9,000+ organizations.Get the eBookStart these courses todayLearn the Databricks Lakehouse Platform at your own pace.\n",
      "Start learningA must-read for ML engineers and data scientists seeking a better way to do MLOps.Get the eBookReady to get started?Try Databricks for free\n",
      "\n",
      "ProductPlatform OverviewPricingOpen Source TechTry DatabricksDemoProductPlatform OverviewPricingOpen Source TechTry DatabricksDemoLearn & SupportDocumentationGlossaryTraining & CertificationHelp CenterLegalOnline CommunityLearn & SupportDocumentationGlossaryTraining & CertificationHelp CenterLegalOnline CommunitySolutionsBy IndustriesProfessional ServicesSolutionsBy IndustriesProfessional ServicesCompanyAbout UsCareers at DatabricksDiversity and InclusionCompany BlogContact UsCompanyAbout UsCareers at DatabricksDiversity and InclusionCompany BlogContact UsSee Careers\n",
      "at DatabricksWorldwideEnglish (United States)Deutsch (Germany)Français (France)Italiano (Italy)日本語 (Japan)한국어 (South Korea)Português (Brazil)LinkedInFacebookTwitterRssGlassdoorYoutubeDatabricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121© Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.Privacy Notice|Terms of Use|Your Privacy Choices|Your California Privacy Rights\n",
      "type of doc_example: <class 'langchain.schema.document.Document'>\n"
     ]
    }
   ],
   "source": [
    "# describe the documents\n",
    "print('type of documents:', type(documents))\n",
    "print('shape of documents:', len(documents))\n",
    "doc_example = documents[0]\n",
    "print('doc_example :', doc_example.page_content)\n",
    "print('type of doc_example:', type(doc_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings( model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# storing embeddings in the vector store\n",
    "vectorstore = FAISS.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In Databricks, the Medallion architecture refers to a multi-layered approach to validating, cleaning, and transforming data for analytics. This includes data catalogs, data pipelines, data warehousing, and data governance.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Data lakehouse architecture in Databricks?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='What is data modeling on Databricks? \\nThe Databricks Lakehouse Platform organizes data stored with Delta Lake in cloud object storage with familiar relations like database schemas, tables, and views. Databricks recommends a multi-layer approach to validating, cleansing, and transforming data for analytics. For more information, see the medallion architecture.\\n\\n\\nWhat is Databricks SQL? \\nDatabricks SQL provides general compute resources for SQL queries, visualizations, and dashboards that are executed against the tables in the lakehouse. Within Databricks SQL, these queries, visualizations, and dashboards are developed and executed using SQL editor.', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution in Databricks using Databricks SQL.', 'language': 'en-US'}), Document(page_content='Data governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Sep 08, 2023\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nSecurity and compliance guide\\n\\n\\n\\n\\n\\n\\n\\nSecurity and compliance guide \\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their Databricks environment according to their risk profile and governance policy.\\nThis guide does not cover information about securing your data. For that information, see Data governance best practices.\\n\\nNote\\nThis article focuses on the most recent (E2) version of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}), Document(page_content='What is the Databricks Lakehouse? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\n  What is the Databricks Lakehouse?\\nWhat are ACID guarantees on Databricks?\\nWhat is the medallion lakehouse architecture?\\nWhat does it mean to build a single source of truth?\\nData discovery and collaboration in the lakehouse\\nData objects in the Databricks Lakehouse\\n\\n\\n  What is Delta?\\n  Concepts\\n  Architecture\\n  Integrations\\n\\n\\nTutorials and best practices\\nRelease notes\\n\\nLoad & manage data\\n\\nLoad data\\nExplore data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is the Databricks Lakehouse? | Databricks on AWS', 'description': 'Use the Databricks Lakehouse for ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}), Document(page_content='Data preparation resources and information \\nThe Databricks Lakehouse provides a unified platform for data ingestion, preparation, analytics and machine learning, and monitoring.', metadata={'source': 'http://docs.databricks.com/data-preparation/index.html', 'title': 'Introduction to data preparation in Databricks | Databricks on AWS', 'description': 'This article provides an introduction to tools and techniques for data preparation in Databricks. Learn about tools and processes to streamline data preparation.', 'language': 'en-US'})]\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  In a Data Lakehouse architecture, Data Governance plays a crucial role in ensuring the secure management of data assets within an organization. It encompasses policies and practices that regulate how data is collected, stored, processed, and used across various departments and stakeholders. In Databricks, Data Governance is particularly important as it helps to simplify data governance by unifying data warehousing and AI use cases on a single platform. By implementing centralized data governance, organizations can better manage their data assets, reduce the risk of data breaches, and ensure compliance with regulatory requirements.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "\n",
    "query = \"What are Data Governance and Interoperability in it?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])\n",
    "chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is databricks what solutions does it provide?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to the provided text, Databricks provides the following solutions:\n",
      "\n",
      "1. Build an enterprise data lakehouse.\n",
      "2. Important: This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.\n",
      "3. Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.\n",
      "4. Databricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies.\n",
      "5. Databricks includes Partner Connect, a user interface that allows validated solutions to integrate more quickly and easily with your Databricks clusters and SQL warehouses.\n",
      "6. Build an integration with Databricks.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Databricks documentation archive \\n\\nImportant\\nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.\\n\\nIn this archive, you can find earlier versions of documentation for Databricks products, features, APIs, and workflows.', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and may not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'}), Document(page_content='Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.\\nUnity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.\\nDatabricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies. Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': '‘Learn what Databricks is and what it is used for: tools and use cases of the Databricks Lakehouse Platform.’', 'language': 'en-US'}), Document(page_content='Technology partners \\nDatabricks has validated integrations with various third-party solutions that allow you to work with data through Databricks clusters and SQL warehouses, in many cases with low-code and no-code experiences. These solutions enable common scenarios such as data ingestion, data preparation and transformation, business intelligence (BI), and machine learning.\\nDatabricks also includes Partner Connect, a user interface that allows some of these validated solutions to integrate more quickly and easily with your Databricks clusters and SQL warehouses.\\n\\n\\nBuild an integration with Databricks \\nThis section provides instructions and best practices for technology partners to build and maintain their integrations with Databricks.\\n\\nBest practices for ingestion partners using Unity Catalog volumes as staging locations for data', metadata={'source': 'http://docs.databricks.com/integrations/index.html', 'title': 'Technology partners | Databricks on AWS', 'description': 'Learn how you can connect technology partners to your Databricks workspace so you can use third-party tools with your Lakehouse data.', 'language': 'en-US'}), Document(page_content='What are common use cases for Databricks? \\nUse cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.\\n\\n\\nBuild an enterprise data lakehouse \\nThe data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See What is the Databricks Lakehouse?.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': '‘Learn what Databricks is and what it is used for: tools and use cases of the Databricks Lakehouse Platform.’', 'language': 'en-US'})]\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'what is databricks what solutions does it provide?', 'chat_history': [('What is Data lakehouse architecture in Databricks?', ' In Databricks, the Medallion architecture refers to a multi-layered approach to validating, cleaning, and transforming data for analytics. This includes data catalogs, data pipelines, data warehousing, and data governance.'), ('What are Data Governance and Interoperability in it?', '  In a Data Lakehouse architecture, Data Governance plays a crucial role in ensuring the secure management of data assets within an organization. It encompasses policies and practices that regulate how data is collected, stored, processed, and used across various departments and stakeholders. In Databricks, Data Governance is particularly important as it helps to simplify data governance by unifying data warehousing and AI use cases on a single platform. By implementing centralized data governance, organizations can better manage their data assets, reduce the risk of data breaches, and ensure compliance with regulatory requirements.')], 'answer': ' According to the provided text, Databricks provides the following solutions:\\n\\n1. Build an enterprise data lakehouse.\\n2. Important: This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.\\n3. Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.\\n4. Databricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies.\\n5. Databricks includes Partner Connect, a user interface that allows validated solutions to integrate more quickly and easily with your Databricks clusters and SQL warehouses.\\n6. Build an integration with Databricks.', 'source_documents': [Document(page_content='Databricks documentation archive \\n\\nImportant\\nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.\\n\\nIn this archive, you can find earlier versions of documentation for Databricks products, features, APIs, and workflows.', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and may not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'}), Document(page_content='Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.\\nUnity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.\\nDatabricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies. Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': '‘Learn what Databricks is and what it is used for: tools and use cases of the Databricks Lakehouse Platform.’', 'language': 'en-US'}), Document(page_content='Technology partners \\nDatabricks has validated integrations with various third-party solutions that allow you to work with data through Databricks clusters and SQL warehouses, in many cases with low-code and no-code experiences. These solutions enable common scenarios such as data ingestion, data preparation and transformation, business intelligence (BI), and machine learning.\\nDatabricks also includes Partner Connect, a user interface that allows some of these validated solutions to integrate more quickly and easily with your Databricks clusters and SQL warehouses.\\n\\n\\nBuild an integration with Databricks \\nThis section provides instructions and best practices for technology partners to build and maintain their integrations with Databricks.\\n\\nBest practices for ingestion partners using Unity Catalog volumes as staging locations for data', metadata={'source': 'http://docs.databricks.com/integrations/index.html', 'title': 'Technology partners | Databricks on AWS', 'description': 'Learn how you can connect technology partners to your Databricks workspace so you can use third-party tools with your Lakehouse data.', 'language': 'en-US'}), Document(page_content='What are common use cases for Databricks? \\nUse cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.\\n\\n\\nBuild an enterprise data lakehouse \\nThe data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See What is the Databricks Lakehouse?.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': '‘Learn what Databricks is and what it is used for: tools and use cases of the Databricks Lakehouse Platform.’', 'language': 'en-US'})]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
