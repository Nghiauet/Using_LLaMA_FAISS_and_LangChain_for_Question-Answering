{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:1006: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.95s/it]\n",
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
       " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = generate_text(\"Explain me the difference between Data Lakehouse and Data Warehouse.\")\n",
    "# print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing HF Pipeline in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " everybody has a different definition of success, but here are some common traits and strategies that successful entrepreneurs tend to have:\n",
      "\n",
      "1. Passion and drive: Successful entrepreneurs are often driven by a deep passion for their business or industry. They have a clear vision for what they want to achieve and are willing to put in the hard work necessary to make it happen.\n",
      "2. Adaptability: The ability to adapt quickly is crucial for entrepreneurs. As markets change and new challenges arise, successful entrepreneurs are able to pivot and adjust their strategy accordingly.\n",
      "3. Resilience: Entrepreneurship can be a difficult and unpredictable journey. Successful entrepreneurs have the resilience to bounce back from setbacks and failures, learning from each experience and using it as an opportunity for growth.\n",
      "4. Networking skills: Building a strong network of contacts and connections is essential for entrepreneurs. Successful entrepreneurs know how to build and leverage these networks to access resources, gain support, and stay informed about market trends.\n",
      "5. Focus on customer needs: Successful entrepreneurs prioritize meeting the needs of their customers. They understand their target market and are constantly looking for ways to improve their products or services to better serve those customers.\n",
      "6. Financial management: Managing finances effectively is critical for entrepreneurs. Successful entrepreneurs know how to budget, manage cash flow, and make smart financial decisions that support the growth of their business.\n",
      "7. Innovation: Successful entrepreneurs are always looking for ways to innovate and differentiate themselves from competitors. They invest time and resources into research and development, seeking out new ideas and technologies that can help them stay ahead of the curve.\n",
      "8. Visionary leadership: Great entrepreneurs have a clear vision for their business and inspire others to follow it. They lead by example, motivating and empowering their teams to achieve shared goals.\n",
      "9. Risk tolerance: Starting a business involves taking calculated risks, and successful entrepreneurs are comfortable with taking risks. They weigh the potential rewards against the potential costs and make informed decisions based on their analysis.\n",
      "10. Continuous learning: Finally, successful entrepreneurs recognize that there is always more to learn and grow. They continue to educate themselves through\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# checking again that everything is working fine\n",
    "text = llm(prompt=\"How to succeed in entrepreneurship?\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web_links = [\"https://www.databricks.com/\",\"https://help.databricks.com\",\"https://databricks.com/try-databricks\",\"https://help.databricks.com/s/\",\"https://docs.databricks.com\",\"https://kb.databricks.com/\",\"http://docs.databricks.com/getting-started/index.html\",\"http://docs.databricks.com/introduction/index.html\",\"http://docs.databricks.com/getting-started/tutorials/index.html\",\"http://docs.databricks.com/release-notes/index.html\",\"http://docs.databricks.com/ingestion/index.html\",\"http://docs.databricks.com/exploratory-data-analysis/index.html\",\"http://docs.databricks.com/data-preparation/index.html\",\"http://docs.databricks.com/data-sharing/index.html\",\"http://docs.databricks.com/marketplace/index.html\",\"http://docs.databricks.com/workspace-index.html\",\"http://docs.databricks.com/machine-learning/index.html\",\"http://docs.databricks.com/sql/index.html\",\"http://docs.databricks.com/delta/index.html\",\"http://docs.databricks.com/dev-tools/index.html\",\"http://docs.databricks.com/integrations/index.html\",\"http://docs.databricks.com/administration-guide/index.html\",\"http://docs.databricks.com/security/index.html\",\"http://docs.databricks.com/data-governance/index.html\",\"http://docs.databricks.com/lakehouse-architecture/index.html\",\"http://docs.databricks.com/reference/api.html\",\"http://docs.databricks.com/resources/index.html\",\"http://docs.databricks.com/whats-coming.html\",\"http://docs.databricks.com/archive/index.html\",\"http://docs.databricks.com/lakehouse/index.html\",\"http://docs.databricks.com/getting-started/quick-start.html\",\"http://docs.databricks.com/getting-started/etl-quick-start.html\",\"http://docs.databricks.com/getting-started/lakehouse-e2e.html\",\"http://docs.databricks.com/getting-started/free-training.html\",\"http://docs.databricks.com/sql/language-manual/index.html\",\"http://docs.databricks.com/error-messages/index.html\",\"http://www.apache.org/\",\"https://databricks.com/privacy-policy\",\"https://databricks.com/terms-of-use\"] \n",
    "\n",
    "loader = WebBaseLoader(web_links)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of documents: <class 'list'>\n",
      "shape of documents: 39\n",
      "doc_example : Data Lakehouse Architecture and AI Company | DatabricksSkip to main contentPlatformThe Databricks Lakehouse PlatformDelta LakeData GovernanceData EngineeringData StreamingData WarehousingData SharingMachine LearningData SciencePricingMarketplaceOpen source techSecurity and Trust CenterDiscover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform\n",
      "Read nowSolutionsSolutions by IndustryFinancial ServicesHealthcare and Life SciencesManufacturingCommunications, Media & EntertainmentPublic SectorRetailSee all IndustriesSolutions by Use CaseSolution AcceleratorsProfessional ServicesDigital Native BusinessesData Platform MigrationReport\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report\n",
      "Read nowLearnDocumentationTraining & CertificationDemosResourcesOnline CommunityUniversity AllianceEventsData + AI SummitBlogLabsBeaconsExecutive InsightsMissed Data + AI Summit?\n",
      " \n",
      "Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand.Watch on demandCustomersPartnersCloud PartnersAWSAzureGoogle CloudPartner ConnectTechnology and Data PartnersTechnology Partner ProgramData Partner ProgramBuilt on Databricks Partner ProgramConsulting & SI PartnersC&SI Partner ProgramPartner SolutionsConnect with validated partner solutions in just a few clicks.Learn moreCompanyCareers at DatabricksOur TeamBoard of DirectorsCompany BlogNewsroomDatabricks VenturesAwards and RecognitionContact UsSee why Gartner named Databricks a Leader for the second consecutive yearGet the reportTry DatabricksWatch DemosContact UsLoginJoin Generation AIConnect with like-minded peers and companies who believe in the transformative power of data, analytics and AI.Explore eventsThe best data warehouse is a lakehouseUnify all your data, analytics and\n",
      "AI on one platformGet started for freeLearn moreCut costs and speed up innovation with the Lakehouse PlatformLearn moreUnifiedOne platform for your data, consistently governed and available for all your analytics and AIOpenOpen standards provide easy integration with other tools plus secure, platform-independent data sharingScalableScale efficiently with every workload from simple data pipelines to massive LLMsData-driven organizations choosing Lakehouse\n",
      "See all customers\n",
      "Lakehouse unifies your data teamsData sharingOpen data sharingEasily collaborate with anyone on any platform with the first open approach to data sharing. Share live data sets, models, dashboards and notebooks while maintaining strict security and governance.Learn more Watch demoData management and engineeringStreamline your data ingestion and managementWith automated and reliable ETL, open and secure data sharing, and lightning-fast performance, Delta Lake transforms your data lake into the destination for all your structured, semi-structured and unstructured data.Learn more Watch demoData warehousingDerive new insights from the most complete dataWith ready access to the freshest and most complete data and the power of Databricks SQL — up to 12x better price/performance than traditional cloud data warehouses \u0000— data analysts and scientists can now quickly derive new insights.Learn more Watch demoData science and machine learningAccelerate ML across the entire lifecycleThe lakehouse forms the foundation of Databricks Machine Learning — a data-native and collaborative solution for the full machine learning lifecycle, from featurization to production. Combined with high-quality, highly performant data pipelines, lakehouse accelerates machine learning and team productivity.Learn more Watch demoData governanceUnify governance for data, analytics and AIMaintain a compliant, end-to-end view of your data estate with a single model of data governance for all your structured and unstructured data. Centralize auditing and track usage through automated lineage and monitoring capabilities.Learn more Watch demoThe data warehouse is history. Discover why the lakehouse is the modern architecture for data and AI.Discover LakehouseJoin Generation AIAugust–November 2023Explore the latest in data, analytics and AI on Databricks Lakehouse. Connect with local data teams at a Data + AI World Tour stop near you.Register nowData + AI in the real worldDiscover the latest trends in data science and AI adoption across 9,000+ organizations.Get the eBookStart these courses todayLearn the Databricks Lakehouse Platform at your own pace.\n",
      "Start learningA must-read for ML engineers and data scientists seeking a better way to do MLOps.Get the eBookReady to get started?Try Databricks for free\n",
      "\n",
      "ProductPlatform OverviewPricingOpen Source TechTry DatabricksDemoProductPlatform OverviewPricingOpen Source TechTry DatabricksDemoLearn & SupportDocumentationGlossaryTraining & CertificationHelp CenterLegalOnline CommunityLearn & SupportDocumentationGlossaryTraining & CertificationHelp CenterLegalOnline CommunitySolutionsBy IndustriesProfessional ServicesSolutionsBy IndustriesProfessional ServicesCompanyAbout UsCareers at DatabricksDiversity and InclusionCompany BlogContact UsCompanyAbout UsCareers at DatabricksDiversity and InclusionCompany BlogContact UsSee Careers\n",
      "at DatabricksWorldwideEnglish (United States)Deutsch (Germany)Français (France)Italiano (Italy)日本語 (Japan)한국어 (South Korea)Português (Brazil)LinkedInFacebookTwitterRssGlassdoorYoutubeDatabricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121© Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.Privacy Notice|Terms of Use|Your Privacy Choices|Your California Privacy Rights\n",
      "type of doc_example: <class 'langchain.schema.document.Document'>\n"
     ]
    }
   ],
   "source": [
    "# describe the documents\n",
    "print('type of documents:', type(documents))\n",
    "print('shape of documents:', len(documents))\n",
    "doc_example = documents[0]\n",
    "print('doc_example :', doc_example.page_content)\n",
    "print('type of doc_example:', type(doc_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings( model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# storing embeddings in the vector store\n",
    "vectorstore = FAISS.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The medallion lakehouse architecture is a multi-layer approach to validating, cleansing, and transforming data for analytics. It includes the following layers:\n",
      "1. Data ingestion layer: This layer handles the ingestion of raw data from various sources into the lakehouse.\n",
      "2. Data storage layer: This layer stores the ingested data in a scalable and durable manner, using technologies such as Apache Hadoop and Amazon S3.\n",
      "3. Data processing layer: This layer processes the stored data using various techniques, including data transformation, aggregation, and filtering.\n",
      "4. Data governance layer: This layer enforces data governance policies and standards across the organization, ensuring data quality and consistency.\n",
      "5. Data visualization and analytics layer: This layer provides tools for visualizing and analyzing the processed data, enabling insights and decision-making.\n",
      "6. Machine learning and AI layer: This layer enables the use of machine learning and artificial intelligence techniques on the processed data, allowing for predictive modeling and automation.\n",
      "7. Monitoring and alerting layer: This layer monitors the lakehouse for performance, security, and compliance issues, providing real-time alerts and notifications.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Data lakehouse architecture in Databricks?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='What is data modeling on Databricks? \\nThe Databricks Lakehouse Platform organizes data stored with Delta Lake in cloud object storage with familiar relations like database schemas, tables, and views. Databricks recommends a multi-layer approach to validating, cleansing, and transforming data for analytics. For more information, see the medallion architecture.\\n\\n\\nWhat is Databricks SQL? \\nDatabricks SQL provides general compute resources for SQL queries, visualizations, and dashboards that are executed against the tables in the lakehouse. Within Databricks SQL, these queries, visualizations, and dashboards are developed and executed using SQL editor.', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution in Databricks using Databricks SQL.', 'language': 'en-US'}), Document(page_content='Data governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Sep 08, 2023\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nSecurity and compliance guide\\n\\n\\n\\n\\n\\n\\n\\nSecurity and compliance guide \\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their Databricks environment according to their risk profile and governance policy.\\nThis guide does not cover information about securing your data. For that information, see Data governance best practices.\\n\\nNote\\nThis article focuses on the most recent (E2) version of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}), Document(page_content='What is the Databricks Lakehouse? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\n  What is the Databricks Lakehouse?\\nWhat are ACID guarantees on Databricks?\\nWhat is the medallion lakehouse architecture?\\nWhat does it mean to build a single source of truth?\\nData discovery and collaboration in the lakehouse\\nData objects in the Databricks Lakehouse\\n\\n\\n  What is Delta?\\n  Concepts\\n  Architecture\\n  Integrations\\n\\n\\nTutorials and best practices\\nRelease notes\\n\\nLoad & manage data\\n\\nLoad data\\nExplore data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is the Databricks Lakehouse? | Databricks on AWS', 'description': 'Use the Databricks Lakehouse for ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}), Document(page_content='Data preparation resources and information \\nThe Databricks Lakehouse provides a unified platform for data ingestion, preparation, analytics and machine learning, and monitoring.', metadata={'source': 'http://docs.databricks.com/data-preparation/index.html', 'title': 'Introduction to data preparation in Databricks | Databricks on AWS', 'description': 'This article provides an introduction to tools and techniques for data preparation in Databricks. Learn about tools and processes to streamline data preparation.', 'language': 'en-US'})]\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! In a Data Lakehouse architecture, data governance plays a crucial role in ensuring the quality, security, and compliance of data. It involves establishing policies, procedures, and standards for managing data across various stakeholders, including data creators, data consumers, and data stewards.\n",
      "Some key aspects of data governance in a Data Lakehouse include:\n",
      "\n",
      "* Data quality management: Ensuring that data is accurate, complete, and consistent across different sources and systems.\n",
      "* Data security management: Implementing access controls, encryption, and other security measures to protect sensitive data from unauthorized access or breaches.\n",
      "* Compliance management: Ensuring that data is handled in accordance with relevant laws, regulations, and industry standards, such as GDPR, HIPAA, or CCPA.\n",
      "* Data lineage management: Tracking the origin, movement, and usage of data throughout its lifecycle to maintain transparency and accountability.\n",
      "* Data catalog management: Creating and maintaining a comprehensive inventory of data assets, including metadata and descriptive information.\n",
      "By implementing effective data governance practices, organizations can build trust in their Data Lakehouse architecture, improve data quality, reduce risks associated with data breaches or non-compliance, and ultimately drive better decision-making through more reliable and secure data insights.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "\n",
    "query = \"What are Data Governance and Interoperability in it?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])\n",
    "chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is databricks what solutions does it provide?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Databricks is a cloud-based platform that provides various solutions for data engineering, data warehousing, and data science. Its main features include data ingestion, data transformation, data visualization, and data governance. Additionally, Databricks offers a range of tools and services for building and deploying machine learning models, as well as integrating with popular programming languages such as Python and R. Overall, Databricks aims to simplify the process of working with large datasets and enabling organizations to extract insights and value from their data.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Databricks provides a number of custom tools for data ingestion, including Auto Loader, an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': '‘Learn what Databricks is and what it is used for: tools and use cases of the Databricks Lakehouse Platform.’', 'language': 'en-US'}), Document(page_content='What is data modeling on Databricks? \\nThe Databricks Lakehouse Platform organizes data stored with Delta Lake in cloud object storage with familiar relations like database schemas, tables, and views. Databricks recommends a multi-layer approach to validating, cleansing, and transforming data for analytics. For more information, see the medallion architecture.\\n\\n\\nWhat is Databricks SQL? \\nDatabricks SQL provides general compute resources for SQL queries, visualizations, and dashboards that are executed against the tables in the lakehouse. Within Databricks SQL, these queries, visualizations, and dashboards are developed and executed using SQL editor.', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution in Databricks using Databricks SQL.', 'language': 'en-US'}), Document(page_content='Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.\\nUnity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.\\nDatabricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies. Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': '‘Learn what Databricks is and what it is used for: tools and use cases of the Databricks Lakehouse Platform.’', 'language': 'en-US'}), Document(page_content='Databricks documentation archive \\n\\nImportant\\nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.\\n\\nIn this archive, you can find earlier versions of documentation for Databricks products, features, APIs, and workflows.', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and may not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'})]\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'what is databricks what solutions does it provide?', 'chat_history': [('What is Data lakehouse architecture in Databricks?', ' The medallion lakehouse architecture is a multi-layer approach to validating, cleansing, and transforming data for analytics. It includes the following layers:\\n1. Data ingestion layer: This layer handles the ingestion of raw data from various sources into the lakehouse.\\n2. Data storage layer: This layer stores the ingested data in a scalable and durable manner, using technologies such as Apache Hadoop and Amazon S3.\\n3. Data processing layer: This layer processes the stored data using various techniques, including data transformation, aggregation, and filtering.\\n4. Data governance layer: This layer enforces data governance policies and standards across the organization, ensuring data quality and consistency.\\n5. Data visualization and analytics layer: This layer provides tools for visualizing and analyzing the processed data, enabling insights and decision-making.\\n6. Machine learning and AI layer: This layer enables the use of machine learning and artificial intelligence techniques on the processed data, allowing for predictive modeling and automation.\\n7. Monitoring and alerting layer: This layer monitors the lakehouse for performance, security, and compliance issues, providing real-time alerts and notifications.'), ('What are Data Governance and Interoperability in it?', '  Of course! In a Data Lakehouse architecture, data governance plays a crucial role in ensuring the quality, security, and compliance of data. It involves establishing policies, procedures, and standards for managing data across various stakeholders, including data creators, data consumers, and data stewards.\\nSome key aspects of data governance in a Data Lakehouse include:\\n\\n* Data quality management: Ensuring that data is accurate, complete, and consistent across different sources and systems.\\n* Data security management: Implementing access controls, encryption, and other security measures to protect sensitive data from unauthorized access or breaches.\\n* Compliance management: Ensuring that data is handled in accordance with relevant laws, regulations, and industry standards, such as GDPR, HIPAA, or CCPA.\\n* Data lineage management: Tracking the origin, movement, and usage of data throughout its lifecycle to maintain transparency and accountability.\\n* Data catalog management: Creating and maintaining a comprehensive inventory of data assets, including metadata and descriptive information.\\nBy implementing effective data governance practices, organizations can build trust in their Data Lakehouse architecture, improve data quality, reduce risks associated with data breaches or non-compliance, and ultimately drive better decision-making through more reliable and secure data insights.')], 'answer': ' Databricks is a cloud-based platform that provides various solutions for data engineering, data warehousing, and data science. Its main features include data ingestion, data transformation, data visualization, and data governance. Additionally, Databricks offers a range of tools and services for building and deploying machine learning models, as well as integrating with popular programming languages such as Python and R. Overall, Databricks aims to simplify the process of working with large datasets and enabling organizations to extract insights and value from their data.', 'source_documents': [Document(page_content='Databricks provides a number of custom tools for data ingestion, including Auto Loader, an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': '‘Learn what Databricks is and what it is used for: tools and use cases of the Databricks Lakehouse Platform.’', 'language': 'en-US'}), Document(page_content='What is data modeling on Databricks? \\nThe Databricks Lakehouse Platform organizes data stored with Delta Lake in cloud object storage with familiar relations like database schemas, tables, and views. Databricks recommends a multi-layer approach to validating, cleansing, and transforming data for analytics. For more information, see the medallion architecture.\\n\\n\\nWhat is Databricks SQL? \\nDatabricks SQL provides general compute resources for SQL queries, visualizations, and dashboards that are executed against the tables in the lakehouse. Within Databricks SQL, these queries, visualizations, and dashboards are developed and executed using SQL editor.', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution in Databricks using Databricks SQL.', 'language': 'en-US'}), Document(page_content='Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.\\nUnity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.\\nDatabricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies. Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': '‘Learn what Databricks is and what it is used for: tools and use cases of the Databricks Lakehouse Platform.’', 'language': 'en-US'}), Document(page_content='Databricks documentation archive \\n\\nImportant\\nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.\\n\\nIn this archive, you can find earlier versions of documentation for Databricks products, features, APIs, and workflows.', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and may not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'})]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
