{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers\n",
    "# !pip install python-dotenv\n",
    "\n",
    "# load variable from .env \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:1006: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "hf_auth  = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghiaph/GEC/GEC_env/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
       " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = generate_text(\"Explain me the difference between Data Lakehouse and Data Warehouse.\")\n",
    "# print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing HF Pipeline in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " nobody knows the answer to that question better than Elon Musk. As the CEO of SpaceX and Tesla, he has built two of the most successful and innovative companies of our time. And yet, despite his incredible success, he still faces challenges and setbacks on a daily basis. In this article, we will explore some of the key lessons that Musk has learned throughout his career, and how they can help you succeed in entrepreneurship.\n",
      "1. Think Big: One of the most important lessons that Musk has learned is the importance of thinking big. He believes that the only way to truly make an impact is to aim high and push the boundaries of what is possible. \"I think it's important to have a vision for the future that is much more ambitious than what we have today,\" Musk says. \"We need to be thinking about how to solve some of the world's biggest problems, like climate change and sustainable energy.\"\n",
      "2. Take Risks: Another key lesson that Musk has learned is the importance of taking risks. He believes that the only way to achieve greatness is to be willing to take calculated risks and embrace uncertainty. \"I think it's important to be willing to take risks and embrace uncertainty,\" Musk says. \"That's where the biggest rewards are often found.\"\n",
      "3. Learn from Failure: Musk has also learned the importance of learning from failure. He believes that every failure is an opportunity to learn and grow, and that the best entrepreneurs are those who can turn failure into success. \"I think it's important to view failure as a learning experience rather than something to be feared,\" Musk says. \"The best entrepreneurs are those who can take risks and learn from their mistakes.\"\n",
      "4. Build a Strong Team: Musk has also learned the importance of building a strong team. He believes that no one person can achieve greatness alone, and that the best entrepreneurs are those who can surround themselves with talented and dedicated individuals. \"I think it's important to build a team of people who share your vision and are committed to helping you achieve it,\" Musk says. \"The best entrepreneurs are those who can inspire and motivate their teams to achieve great things.\"\n",
      "5. Stay Focused: Finally, Musk\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# checking again that everything is working fine\n",
    "text = llm(prompt=\"How to succeed in entrepreneurship?\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web_links = [\"https://www.databricks.com/\",\"https://help.databricks.com\",\"https://databricks.com/try-databricks\",\"https://help.databricks.com/s/\",\"https://docs.databricks.com\",\"https://kb.databricks.com/\",\"http://docs.databricks.com/getting-started/index.html\",\"http://docs.databricks.com/introduction/index.html\",\"http://docs.databricks.com/getting-started/tutorials/index.html\",\"http://docs.databricks.com/release-notes/index.html\",\"http://docs.databricks.com/ingestion/index.html\",\"http://docs.databricks.com/exploratory-data-analysis/index.html\",\"http://docs.databricks.com/data-preparation/index.html\",\"http://docs.databricks.com/data-sharing/index.html\",\"http://docs.databricks.com/marketplace/index.html\",\"http://docs.databricks.com/workspace-index.html\",\"http://docs.databricks.com/machine-learning/index.html\",\"http://docs.databricks.com/sql/index.html\",\"http://docs.databricks.com/delta/index.html\",\"http://docs.databricks.com/dev-tools/index.html\",\"http://docs.databricks.com/integrations/index.html\",\"http://docs.databricks.com/administration-guide/index.html\",\"http://docs.databricks.com/security/index.html\",\"http://docs.databricks.com/data-governance/index.html\",\"http://docs.databricks.com/lakehouse-architecture/index.html\",\"http://docs.databricks.com/reference/api.html\",\"http://docs.databricks.com/resources/index.html\",\"http://docs.databricks.com/whats-coming.html\",\"http://docs.databricks.com/archive/index.html\",\"http://docs.databricks.com/lakehouse/index.html\",\"http://docs.databricks.com/getting-started/quick-start.html\",\"http://docs.databricks.com/getting-started/etl-quick-start.html\",\"http://docs.databricks.com/getting-started/lakehouse-e2e.html\",\"http://docs.databricks.com/getting-started/free-training.html\",\"http://docs.databricks.com/sql/language-manual/index.html\",\"http://docs.databricks.com/error-messages/index.html\",\"http://www.apache.org/\",\"https://databricks.com/privacy-policy\",\"https://databricks.com/terms-of-use\"] \n",
    "\n",
    "loader = WebBaseLoader(web_links)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of documents: <class 'list'>\n",
      "shape of documents: 39\n",
      "doc_example : Data Lakehouse Architecture and AI Company | DatabricksSkip to main contentWhy DatabricksWhy Choose Databricks?For executivesFor startups CustomersFeaturedSee all customersPartnersCloud ProvidersTechnology & Data PartnersTechnology PartnersConnect your existing tools to your LakehouseData PartnersAccess the ecosystem of data consumersBuilt on DatabricksBuild, market and grow your businessConsulting & System IntegratorsC&SI Partner ProgramBuild, deploy or migrate to the LakehousePartner SolutionsFind custom industry and migration solutionsSecurity & Trust CenterThe Data LakehouseData + AI in the real worldDiscover the latest trends in data and AI adoption across 9000+ organizationsGet the eBookProductLakehouse PlatformLakehouse Platform OverviewA unified platform for data, analytics and AIData SharingAn open, secure, zero-copy sharing for all dataData GovernanceUnified governance for all data, analytics and AI assetsArtificial IntelligenceBuild, train, and manage end-to-end AI applicationsMosaicMLTrain and deploy secure generative AI modelsData ManagementReliability, security and performance for your dataData WarehousingServerless data warehouse for SQL analyticsData EngineeringETL and orchestration for batch and streaming dataData StreamingReal-time analytics, AI and applications made simpleData ScienceCollaborative data science at scalePricingIntegrations & DataMarketplaceExplore an open marketplace for data, analytics and AIIDE IntegrationsBuild on the Lakehouse in your favorite IDEPartner ConnectDiscover and integrate with the Databricks ecosystemOpen SourceOpen Source OverviewLearn more about the innovations behind the platformApache SparkA unified engine for large-scale data analyticsDelta LakeAn open source storage framework for data lakehousesMLFlowAn open source platform for the machine learning lifecycleDollyAn open source, instruction-following LLMMarketplaceVirtual EventAdvantage Lakehouse: Fueling Innovation in Data and AIRegister nowSolutionsLakehouse for IndustriesFinancial ServicesHealthcare & Life SciencesManufacturingCommunications, Media and EntertainmentPublic SectorRetailSee AllSolution AcceleratorsData MigrationDatabricks Professional ServicesReportScaling AI: Key Data Priorities\n",
      "Findings from 600 CIOs across 14 industriesRead nowResourcesTraining & CertificationLearning OverviewHub for training, certification, events and moreTrainingDiscover curriculum tailored to your needsLearning PathsGuided learning by role and career pathCertificationGain recognition and differentiationUniversity AllianceFor students and educatorsDatabricks AcademySign in to the Databricks learning platformEventsBlog & PodcastsDatabricks BlogSee what's newData Brew PodcastLet’s talk data!Champions of Data + AI PodcastInsights from data leaders powering innovationCustomer SupportDocumentationCommunityResource CenterAugust–November 2023Generation AI is coming to you! Join us to explore all things data, analytics and AI on Lakehouse.Register nowAboutCompanyWho We AreOur TeamCareersCareers OverviewOpen JobsPress & RecognitionAwards & RecognitionNewsroomDatabricks VenturesContact UsGenerative AI FundamentalsBuild foundational knowledge, including how to use LLMs in your organization, with 4 videosGet startedReady to get started?Get a DemoLoginContact UsTry DatabricksJoin Generation AIConnect with like-minded peers and companies who believe in the transformative power of data, analytics and AI.Explore eventsThe best data warehouse is a lakehouseUnify all your data, analytics and\n",
      "AI on one platformGet started for freeLearn moreCut costs and speed up innovation with the Lakehouse PlatformLearn moreUnifiedOne platform for your data, consistently governed and available for all your analytics and AIOpenOpen standards provide easy integration with other tools plus secure, platform-independent data sharingScalableScale efficiently with every workload from simple data pipelines to massive LLMsData-driven organizations choosing Lakehouse\n",
      "See all customers\n",
      "Lakehouse unifies your data teamsData sharingOpen data sharingEasily collaborate with anyone on any platform with the first open approach to data sharing. Share live data sets, models, dashboards and notebooks while maintaining strict security and governance.Learn more Watch demoData management and engineeringStreamline your data ingestion and managementWith automated and reliable ETL, open and secure data sharing, and lightning-fast performance, Delta Lake transforms your data lake into the destination for all your structured, semi-structured and unstructured data.Learn more Watch demoData warehousingDerive new insights from the most complete dataWith ready access to the freshest and most complete data and the power of Databricks SQL — up to 12x better price/performance than traditional cloud data warehouses \u0000— data analysts and scientists can now quickly derive new insights.Learn more Watch demoData science and machine learningAccelerate ML across the entire lifecycleThe lakehouse forms the foundation of Databricks Machine Learning — a data-native and collaborative solution for the full machine learning lifecycle, from featurization to production. Combined with high-quality, highly performant data pipelines, lakehouse accelerates machine learning and team productivity.Learn more Watch demoData governanceUnify governance for data, analytics and AIMaintain a compliant, end-to-end view of your data estate with a single model of data governance for all your structured and unstructured data. Centralize auditing and track usage through automated lineage and monitoring capabilities.Learn more Watch demoThe data warehouse is history. Discover why the lakehouse is the modern architecture for data and AI.Discover LakehouseJoin Generation AIAugust–November 2023Explore the latest in data, analytics and AI on Databricks Lakehouse. Connect with local data teams at a Data + AI World Tour stop near you.Register nowData + AI in the real worldDiscover the latest trends in data science and AI adoption across 9,000+ organizations.Get the eBookStart these courses todayLearn the Databricks Lakehouse Platform at your own pace.\n",
      "Start learningA must-read for ML engineers and data scientists seeking a better way to do MLOps.Get the eBookReady to get started?Try Databricks for freeProductPlatform OverviewPricingOpen Source TechTry DatabricksDemoProductPlatform OverviewPricingOpen Source TechTry DatabricksDemoLearn & SupportDocumentationGlossaryTraining & CertificationHelp CenterLegalOnline CommunityLearn & SupportDocumentationGlossaryTraining & CertificationHelp CenterLegalOnline CommunitySolutionsBy IndustriesProfessional ServicesSolutionsBy IndustriesProfessional ServicesCompanyAbout UsCareers at DatabricksDiversity and InclusionCompany BlogContact UsCompanyAbout UsCareers at DatabricksDiversity and InclusionCompany BlogContact UsSee Careers\n",
      "at DatabricksWorldwideEnglish (United States)Deutsch (Germany)Français (France)Italiano (Italy)日本語 (Japan)한국어 (South Korea)Português (Brazil)LinkedInFacebookTwitterRssGlassdoorYoutubeDatabricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121© Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.Privacy Notice|Terms of Use|Your Privacy Choices|Your California Privacy Rights\n",
      "type of doc_example: <class 'langchain.schema.document.Document'>\n"
     ]
    }
   ],
   "source": [
    "# describe the documents\n",
    "print('type of documents:', type(documents))\n",
    "print('shape of documents:', len(documents))\n",
    "doc_example = documents[0]\n",
    "print('doc_example :', doc_example.page_content)\n",
    "print('type of doc_example:', type(doc_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings( model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# storing embeddings in the vector store\n",
    "vectorstore = FAISS.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In Databricks, the medallion lakehouse architecture refers to a multi-layered approach to validating, cleaning, and transforming data for analytics. This architecture includes the following layers:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Data lakehouse architecture in Databricks?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='What is data modeling on Databricks? \\nThe Databricks Lakehouse Platform organizes data stored with Delta Lake in cloud object storage with familiar relations like database schemas, tables, and views. Databricks recommends a multi-layer approach to validating, cleansing, and transforming data for analytics. For more information, see the medallion architecture.\\n\\n\\nWhat is Databricks SQL? \\nDatabricks SQL provides general compute resources for SQL queries, visualizations, and dashboards that are executed against the tables in the lakehouse. Within Databricks SQL, these queries, visualizations, and dashboards are developed and executed using SQL editor.', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution in Databricks using Databricks SQL.', 'language': 'en-US'}), Document(page_content='Data governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Sep 11, 2023\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nSecurity and compliance guide\\n\\n\\n\\n\\n\\n\\n\\nSecurity and compliance guide \\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their Databricks environment according to their risk profile and governance policy.\\nThis guide does not cover information about securing your data. For that information, see Data governance best practices.\\n\\nNote\\nThis article focuses on the most recent (E2) version of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}), Document(page_content='What is the Databricks Lakehouse? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\n  What is the Databricks Lakehouse?\\nWhat are ACID guarantees on Databricks?\\nWhat is the medallion lakehouse architecture?\\nWhat does it mean to build a single source of truth?\\nData discovery and collaboration in the lakehouse\\nData objects in the Databricks Lakehouse\\n\\n\\n  What is Delta?\\n  Concepts\\n  Architecture\\n  Integrations\\n\\n\\nTutorials and best practices\\nRelease notes\\n\\nLoad & manage data\\n\\nLoad data\\nExplore data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is the Databricks Lakehouse? | Databricks on AWS', 'description': 'Use the Databricks Lakehouse for ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}), Document(page_content='Data preparation resources and information \\nThe Databricks Lakehouse provides a unified platform for data ingestion, preparation, analytics and machine learning, and monitoring.', metadata={'source': 'http://docs.databricks.com/data-preparation/index.html', 'title': 'Introduction to data preparation in Databricks | Databricks on AWS', 'description': 'This article provides an introduction to tools and techniques for data preparation in Databricks. Learn about tools and processes to streamline data preparation.', 'language': 'en-US'})]\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No, I don't know the answer to this question as it doesn't relate to any piece of context provided in the prompt.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "\n",
    "query = \"What are Data Governance and Interoperability in it?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])\n",
    "chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is databricks what solutions does it provide?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  There is no one \"best\" way to learn MS SQL, as different people prefer different methods and resources. However, here are some popular ways to learn MS SQL:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='COPY INTO\\nDELETE FROM\\nINSERT INTO\\nINSERT OVERWRITE DIRECTORY\\nINSERT OVERWRITE DIRECTORY with Hive format\\nLOAD DATA\\nMERGE INTO\\nUPDATE\\n\\n\\n\\n\\nData retrieval statements \\nYou use a query to retrieve rows from one or more tables according to the specified clauses. The full syntax\\nand brief description of supported clauses are explained in the Query article.\\nThe related SQL statements SELECT and VALUES are also included in this section.\\n\\n\\nQuery\\nSELECT\\nVALUES\\n\\n\\nDatabricks SQL also provides the ability to generate the logical and physical plan for a query using the EXPLAIN statement.\\n\\n\\nEXPLAIN\\n\\n\\n\\n\\nDelta Lake statements \\nYou use Delta Lake SQL statements to manage tables stored in Delta Lake format:\\n\\n\\nCACHE SELECT\\nCONVERT TO DELTA\\nDESCRIBE HISTORY\\nFSCK REPAIR TABLE\\nGENERATE\\nOPTIMIZE\\nREORG TABLE\\nRESTORE\\nVACUUM\\n\\n\\n\\nFor details on using Delta Lake statements, see What is Delta Lake?.', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}), Document(page_content='Documentation \\nVideos, tutorials, and best practices\\n\\n\\n\\n\\n\\n\\n\\nVideos, tutorials, and best practices \\nDatabricks documentation includes many tutorials, videos, get started articles, and best practices guides.\\n\\nVideo tours \\n\\nVideo: Databricks Data Science and Engineering workspace\\nVideo: Databricks SQL workspace\\nVideo: Databricks Machine Learning workspace\\nVideo: Notebook basics\\n\\n\\n\\nGet started with Databricks Data Science & Engineering \\n\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nRun an end-to-end lakehouse analytics pipeline\\nBuild an end-to-end data pipeline\\n\\n\\n\\nGet started with Databricks Machine Learning \\n\\n10-min tutorials: ML notebooks\\nGet started: MLflow quickstart notebooks\\n\\n\\n\\nGet started with Databricks SQL \\n\\nImport and explore sample dashboards\\nRun and visualize a query', metadata={'source': 'http://docs.databricks.com/getting-started/tutorials/index.html', 'title': 'Videos, tutorials, and best practices | Databricks on AWS', 'description': 'Explore tutorials and best practices guides to help you make the most out of Databricks.', 'language': 'en-US'}), Document(page_content='If you’ve logged into Databricks Academy before, use your existing credentials.\\nIf you’ve never logged into Databricks Academy, a customer account has been created for you, using your Databricks username, usually your work email address. You must reset your password. It may take up to 24 hours for the training pathway to appear in your account.\\n\\n\\nAfter you log into your Databricks Academy account, click  in the top left corner.\\n\\nClick Course Catalog.\\nThe catalogs available to you appear. Databricks Academy organizes groupings of learning content into catalogs, which include courses and learning paths.\\n\\n\\n\\nIf you’ve followed the steps above and do not see the pathways in your account, please file a training support ticket.\\nThe Databricks documentation also provides many tutorials and quickstarts that can help you get up to speed on the platform, both here in the Getting Started section and in other sections:', metadata={'source': 'http://docs.databricks.com/getting-started/free-training.html', 'title': 'Get free Databricks training | Databricks on AWS', 'description': 'Access free training for Databricks.', 'language': 'en-US'}), Document(page_content='Start learningA must-read for ML engineers and data scientists seeking a better way to do MLOps.Get the eBookReady to get started?Try Databricks for freeProductPlatform OverviewPricingOpen Source TechTry DatabricksDemoProductPlatform OverviewPricingOpen Source TechTry DatabricksDemoLearn & SupportDocumentationGlossaryTraining & CertificationHelp CenterLegalOnline CommunityLearn & SupportDocumentationGlossaryTraining & CertificationHelp CenterLegalOnline CommunitySolutionsBy IndustriesProfessional ServicesSolutionsBy IndustriesProfessional ServicesCompanyAbout UsCareers at DatabricksDiversity and InclusionCompany BlogContact UsCompanyAbout UsCareers at DatabricksDiversity and InclusionCompany BlogContact UsSee Careers\\nat DatabricksWorldwideEnglish (United States)Deutsch (Germany)Français (France)Italiano (Italy)日本語 (Japan)한국어 (South Korea)Português (Brazil)LinkedInFacebookTwitterRssGlassdoorYoutubeDatabricks Inc.\\n160 Spear Street, 13th Floor\\nSan Francisco, CA 94105', metadata={'source': 'https://www.databricks.com/', 'title': 'Data Lakehouse Architecture and AI Company | Databricks', 'description': 'Databricks combines data warehouses & data lakes into a lakehouse architecture. Collaborate on all of your data, analytics & AI workloads using one platform.', 'language': 'en-US'})]\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'what is databricks what solutions does it provide?', 'chat_history': [('What is Data lakehouse architecture in Databricks?', ' In Databricks, the medallion lakehouse architecture refers to a multi-layered approach to validating, cleaning, and transforming data for analytics. This architecture includes the following layers:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'), ('What are Data Governance and Interoperability in it?', \"  No, I don't know the answer to this question as it doesn't relate to any piece of context provided in the prompt.\")], 'answer': '  There is no one \"best\" way to learn MS SQL, as different people prefer different methods and resources. However, some popular ways to learn MS SQL include:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'source_documents': [Document(page_content='COPY INTO\\nDELETE FROM\\nINSERT INTO\\nINSERT OVERWRITE DIRECTORY\\nINSERT OVERWRITE DIRECTORY with Hive format\\nLOAD DATA\\nMERGE INTO\\nUPDATE\\n\\n\\n\\n\\nData retrieval statements \\nYou use a query to retrieve rows from one or more tables according to the specified clauses. The full syntax\\nand brief description of supported clauses are explained in the Query article.\\nThe related SQL statements SELECT and VALUES are also included in this section.\\n\\n\\nQuery\\nSELECT\\nVALUES\\n\\n\\nDatabricks SQL also provides the ability to generate the logical and physical plan for a query using the EXPLAIN statement.\\n\\n\\nEXPLAIN\\n\\n\\n\\n\\nDelta Lake statements \\nYou use Delta Lake SQL statements to manage tables stored in Delta Lake format:\\n\\n\\nCACHE SELECT\\nCONVERT TO DELTA\\nDESCRIBE HISTORY\\nFSCK REPAIR TABLE\\nGENERATE\\nOPTIMIZE\\nREORG TABLE\\nRESTORE\\nVACUUM\\n\\n\\n\\nFor details on using Delta Lake statements, see What is Delta Lake?.', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}), Document(page_content='Documentation \\nVideos, tutorials, and best practices\\n\\n\\n\\n\\n\\n\\n\\nVideos, tutorials, and best practices \\nDatabricks documentation includes many tutorials, videos, get started articles, and best practices guides.\\n\\nVideo tours \\n\\nVideo: Databricks Data Science and Engineering workspace\\nVideo: Databricks SQL workspace\\nVideo: Databricks Machine Learning workspace\\nVideo: Notebook basics\\n\\n\\n\\nGet started with Databricks Data Science & Engineering \\n\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nRun an end-to-end lakehouse analytics pipeline\\nBuild an end-to-end data pipeline\\n\\n\\n\\nGet started with Databricks Machine Learning \\n\\n10-min tutorials: ML notebooks\\nGet started: MLflow quickstart notebooks\\n\\n\\n\\nGet started with Databricks SQL \\n\\nImport and explore sample dashboards\\nRun and visualize a query', metadata={'source': 'http://docs.databricks.com/getting-started/tutorials/index.html', 'title': 'Videos, tutorials, and best practices | Databricks on AWS', 'description': 'Explore tutorials and best practices guides to help you make the most out of Databricks.', 'language': 'en-US'}), Document(page_content='If you’ve logged into Databricks Academy before, use your existing credentials.\\nIf you’ve never logged into Databricks Academy, a customer account has been created for you, using your Databricks username, usually your work email address. You must reset your password. It may take up to 24 hours for the training pathway to appear in your account.\\n\\n\\nAfter you log into your Databricks Academy account, click  in the top left corner.\\n\\nClick Course Catalog.\\nThe catalogs available to you appear. Databricks Academy organizes groupings of learning content into catalogs, which include courses and learning paths.\\n\\n\\n\\nIf you’ve followed the steps above and do not see the pathways in your account, please file a training support ticket.\\nThe Databricks documentation also provides many tutorials and quickstarts that can help you get up to speed on the platform, both here in the Getting Started section and in other sections:', metadata={'source': 'http://docs.databricks.com/getting-started/free-training.html', 'title': 'Get free Databricks training | Databricks on AWS', 'description': 'Access free training for Databricks.', 'language': 'en-US'}), Document(page_content='Start learningA must-read for ML engineers and data scientists seeking a better way to do MLOps.Get the eBookReady to get started?Try Databricks for freeProductPlatform OverviewPricingOpen Source TechTry DatabricksDemoProductPlatform OverviewPricingOpen Source TechTry DatabricksDemoLearn & SupportDocumentationGlossaryTraining & CertificationHelp CenterLegalOnline CommunityLearn & SupportDocumentationGlossaryTraining & CertificationHelp CenterLegalOnline CommunitySolutionsBy IndustriesProfessional ServicesSolutionsBy IndustriesProfessional ServicesCompanyAbout UsCareers at DatabricksDiversity and InclusionCompany BlogContact UsCompanyAbout UsCareers at DatabricksDiversity and InclusionCompany BlogContact UsSee Careers\\nat DatabricksWorldwideEnglish (United States)Deutsch (Germany)Français (France)Italiano (Italy)日本語 (Japan)한국어 (South Korea)Português (Brazil)LinkedInFacebookTwitterRssGlassdoorYoutubeDatabricks Inc.\\n160 Spear Street, 13th Floor\\nSan Francisco, CA 94105', metadata={'source': 'https://www.databricks.com/', 'title': 'Data Lakehouse Architecture and AI Company | Databricks', 'description': 'Databricks combines data warehouses & data lakes into a lakehouse architecture. Collaborate on all of your data, analytics & AI workloads using one platform.', 'language': 'en-US'})]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
